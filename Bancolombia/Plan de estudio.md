# Fase 1 – Fundamentos y Herramientas (Semanas 1-3):
## Objetivo: Manejar las herramientas clave para el análisis y procesamiento de datos.
## Temas
* Excel Avanzado
  - Tablas dinámicas, segmentación y filtros avanzados.
  - Fórmulas clave (BUSCARX, ÍNDICE, COINCIDIR, DESREF).
  - Funciones estadísticas y financieras.
  - Power Query básico.
* Python para Data Science
    - Numpy, Pandas (limpieza, manipulación, transformación de datos).
    - Matplotlib, Seaborn (visualización de datos).
    - Manejo de datos con fechas y horas (series de tiempo).
    - Lectura de diferentes formatos (CSV, Excel, SQL).
* Power BI
    - Conexión a fuentes de datos.
    - Transformación de datos con Power Query.
    - Creación de visualizaciones y dashboards interactivos.
    - DAX básico.
## Proyecto práctico
* Crea un repositorio en GitHub llamado Analitica-Bancaria.
* Consigue un dataset público de transacciones financieras o consumo (ej. Kaggle - Credit Card Transactions o datos del Banco Mundial).
* Carga, limpia y analiza los datos en Excel y Python, y crea un dashboard inicial en Power BI.

# Fase 2 – Análisis Estadístico y Ciencia de Datos (Semanas 4-6)
## Objetivo: Entender y aplicar análisis descriptivo, diagnóstico e inferencial.
## Temas
* Estadística descriptiva y visualización
    - Medidas de tendencia central y dispersión.
    - Detección de outliers.
    - Histogramas, boxplots, scatterplots.
* Análisis inferencial
    - Pruebas de hipótesis (t-test, ANOVA, chi-cuadrado).
    - Correlación y regresión lineal simple y múltiple.
    - Introducción a modelos de series de tiempo (ARIMA, Prophet).
* Introducción al Machine Learning
    - Conceptos clave (entrenamiento, validación, overfitting).
    - Scikit-learn: regresión, clasificación, métricas de evaluación.
    - Preprocesamiento de datos (normalización, codificación, imputación).
## Proyecto práctico 
Busca un dataset en kaggle y realiza:
* Análisis descriptivo y diagnóstico en Python.
* Prueba de hipótesis sobre variables relevantes.
* Un modelo predictivo simple (por ejemplo, predecir gasto mensual o riesgo de mora).
* Documenta tus análisis con Jupyter Notebooks y súbelos a tu repositorio.

# Fase 3 – ETL y Data Warehousing (Semanas 7-9)
## Objetivo: Aprender a construir flujos de datos y trabajar con almacenamiento centralizado.
## Temas
* ETL con Python
    - Uso de Pandas para extracción, limpieza y carga.
    - Automatización con scripts programados.
    - Conexión a bases de datos (PostgreSQL, MySQL).
* Data Warehouse básico
    - Conceptos de modelado dimensional (star schema, snowflake).
    - Integración con Power BI.
    - Herramientas como Google BigQuery o AWS Redshift (opcional, para simular entorno empresarial).
## Proyecto práctico
* Crea un pipeline ETL que:
* Extraiga datos de múltiples fuentes (CSV, Excel y base de datos).
* Transforme los datos (limpieza, normalización).
* Los cargue en una base de datos local o en la nube.
* Conecta Power BI a esta base de datos y crea un dashboard actualizado.

# Fase 4 – Machine Learning y Deep Learning Aplicado (Semanas 10-12)
## Objetivo: Implementar modelos avanzados y análisis de series de tiempo para toma de decisiones.
## Temas
* Machine Learning avanzado
    - Random Forest, Gradient Boosting.
    - Métricas de clasificación (precisión, recall, F1-score, ROC-AUC).
* Deep Learning
    - Redes neuronales básicas con TensorFlow o PyTorch.
    - Aplicaciones en datos tabulares.
* Series de tiempo
    - Modelos ARIMA, SARIMA.
    - Prophet para predicciones a futuro.
    - Detección de anomalías en datos temporales.
## Proyecto práctico
* Integra todo:
* ETL → Almacena datos históricos de transacciones.
* Dashboard → Power BI mostrando tendencias y métricas clave.
* Modelo ML → Predecir un KPI bancario (ej. riesgo de impago, gasto estimado).
* Informe final documentado en PDF y GitHub.